	at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:538)
	at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:223)
	at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:404)
	... 12 more
Caused by: java.lang.IllegalArgumentException: Invalid format: "Tue Sep 30 16:55:35 CST 2014"
	at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:754)
	at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:604)
	... 15 more
[2014-10-01 17:45:04,919][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:35,138][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] deleting index
[2014-10-01 18:05:36,782][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] creating index, cause [auto(bulk api)], shards [5]/[1], mappings [_default_]
[2014-10-01 18:05:37,608][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:39,579][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:40,020][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:41,878][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [tamperdetection] (dynamic)
[2014-10-01 18:05:41,887][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [videoloss] (dynamic)
[2014-10-01 18:05:41,896][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:41,898][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [VMD] (dynamic)
[2014-10-01 18:05:41,975][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:42,528][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:43,430][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:43,475][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:48,444][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:54,372][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:05:54,853][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:06:49,449][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:07:00,315][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:08:34,936][INFO ][cluster.metadata         ] [Jeffrey Mace] [logstash-2014.10.01] update_mapping [test] (dynamic)
[2014-10-01 18:54:45,943][WARN ][index.merge.scheduler    ] [Jeffrey Mace] [logstash-2014.10.01][3] failed to merge
java.io.IOException: Map failed: MMapIndexInput(path="/home/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.01/3/index/_hi_es090_0.tim") [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, preventing us to map a chunk of 3054244 bytes. Please review 'ulimit -v', 'ulimit -m' (both should return 'unlimited'), and 'sysctl vm.max_map_count'. More information: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]
	at sun.nio.ch.FileChannelImpl.map(Unknown Source)
	at org.apache.lucene.store.MMapDirectory.map(MMapDirectory.java:224)
	at org.apache.lucene.store.MMapDirectory.openInput(MMapDirectory.java:199)
	at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:172)
	at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
	at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
	at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
	at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:375)
	at org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.<init>(BlockTreeTermsReader.java:123)
	at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:441)
	at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.<init>(BloomFilterPostingsFormat.java:133)
	at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:104)
	at org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat.fieldsProducer(Elasticsearch090PostingsFormat.java:79)
	at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.<init>(PerFieldPostingsFormat.java:197)
	at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:254)
	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:120)
	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:107)
	at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:143)
	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4290)
	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3759)
	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)
	at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:106)
	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)
[2014-10-01 18:54:45,945][WARN ][index.engine.internal    ] [Jeffrey Mace] [logstash-2014.10.01][3] failed engine [merge exception]
[2014-10-01 18:54:46,278][WARN ][cluster.action.shard     ] [Jeffrey Mace] [logstash-2014.10.01][3] sending failed shard for [logstash-2014.10.01][3], node[Yf9x_WMeQ42s2FUvTvKoFg], [P], s[STARTED], indexUUID [dh1MOUniRuim_Gbi8o6jMA], reason [engine failure, message [merge exception][MergeException[java.io.IOException: Map failed: MMapIndexInput(path="/home/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.01/3/index/_hi_es090_0.tim") [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, preventing us to map a chunk of 3054244 bytes. Please review 'ulimit -v', 'ulimit -m' (both should return 'unlimited'), and 'sysctl vm.max_map_count'. More information: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]]; nested: IOException[Map failed: MMapIndexInput(path="/home/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.01/3/index/_hi_es090_0.tim") [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, preventing us to map a chunk of 3054244 bytes. Please review 'ulimit -v', 'ulimit -m' (both should return 'unlimited'), and 'sysctl vm.max_map_count'. More information: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]]; ]]
[2014-10-01 18:54:46,279][WARN ][cluster.action.shard     ] [Jeffrey Mace] [logstash-2014.10.01][3] received shard failed for [logstash-2014.10.01][3], node[Yf9x_WMeQ42s2FUvTvKoFg], [P], s[STARTED], indexUUID [dh1MOUniRuim_Gbi8o6jMA], reason [engine failure, message [merge exception][MergeException[java.io.IOException: Map failed: MMapIndexInput(path="/home/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.01/3/index/_hi_es090_0.tim") [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, preventing us to map a chunk of 3054244 bytes. Please review 'ulimit -v', 'ulimit -m' (both should return 'unlimited'), and 'sysctl vm.max_map_count'. More information: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]]; nested: IOException[Map failed: MMapIndexInput(path="/home/elasticsearch/data/elasticsearch/nodes/0/indices/logstash-2014.10.01/3/index/_hi_es090_0.tim") [this may be caused by lack of enough unfragmented virtual address space or too restrictive virtual memory limits enforced by the operating system, preventing us to map a chunk of 3054244 bytes. Please review 'ulimit -v', 'ulimit -m' (both should return 'unlimited'), and 'sysctl vm.max_map_count'. More information: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html]]; ]]
[2014-10-01 20:29:22,549][DEBUG][action.admin.indices.status] [Jeffrey Mace] [logstash-2014.09.30][1], node[Yf9x_WMeQ42s2FUvTvKoFg], [P], s[STARTED]: failed to executed [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@7b11c2d8]
java.lang.OutOfMemoryError
	at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
	at java.io.UnixFileSystem.getBooleanAttributes(Unknown Source)
	at java.io.File.isDirectory(Unknown Source)
	at org.apache.lucene.store.FSDirectory$1.accept(FSDirectory.java:226)
	at java.io.File.list(Unknown Source)
	at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:223)
	at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
	at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:94)
	at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
	at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
	at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
	at org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:40)
	at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:154)
	at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:63)
	at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
[2014-10-01 21:25:08,587][DEBUG][action.admin.indices.status] [Jeffrey Mace] [logstash-2014.10.01][1], node[Yf9x_WMeQ42s2FUvTvKoFg], [P], s[STARTED]: failed to executed [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@fae8fcc]
java.lang.OutOfMemoryError
	at java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
	at java.io.UnixFileSystem.getBooleanAttributes(Unknown Source)
	at java.io.File.isDirectory(Unknown Source)
	at org.apache.lucene.store.FSDirectory$1.accept(FSDirectory.java:226)
	at java.io.File.list(Unknown Source)
	at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:223)
	at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)
	at org.apache.lucene.store.FileSwitchDirectory.listAll(FileSwitchDirectory.java:87)
	at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
	at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)
	at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
	at org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:40)
	at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:154)
	at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:63)
	at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
